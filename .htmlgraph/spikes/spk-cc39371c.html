<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="htmlgraph-version" content="1.0">
    <title>Design: Hook Integration for Auto-Injection of Documentation</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <article id="spk-cc39371c"
             data-type="spike"
             data-status="todo"
             data-priority="medium"
             data-created="2026-01-02T12:15:57.648127"
             data-updated="2026-01-02T12:15:57.648131" data-spike-type="architectural" data-timebox-hours="4">

        <header>
            <h1>Design: Hook Integration for Auto-Injection of Documentation</h1>
            <div class="metadata">
                <span class="badge status-todo">Todo</span>
                <span class="badge priority-medium">Medium Priority</span>
            </div>
        </header>

    
        <section data-spike-metadata>
            <h3>Spike Metadata</h3>
            <dl>
                <dt>Type</dt>
                <dd>Architectural</dd>
                <dt>Timebox</dt>
                <dd>4 hours</dd>
            </dl>
        </section>
        <section data-findings>
            <h3>Findings</h3>
            <div class="findings-content">
                # Hook Integration Design for Auto-Injection of Documentation

## Executive Summary

This design provides a hook-based system for automatically injecting HtmlGraph documentation into AI agent contexts at strategic points, balancing token efficiency with progressive disclosure principles.

**Key Metrics:**
- **Context Budget**: 10k-20k tokens (5-15% of 200k total)
- **Performance Target**: <50ms latency per injection
- **Typical Session**: ~2,200 tokens (14.8% utilization) over 30 minutes
- **Coverage**: SessionStart + PreToolUse + UserPromptSubmit hooks

---

## 1. Hook Selection Strategy

### Primary Hooks (Implement First)

#### SessionStart - Minimal Context Bootstrap
- **Purpose**: Establish baseline understanding without overwhelming context
- **Token Budget**: 150-250 tokens
- **Trigger**: Every new session
- **Injection**: Quick reference card + active work context
- **Performance Target**: <50ms

#### PreToolUse - Just-In-Time Documentation
- **Purpose**: Inject relevant docs right before tool execution
- **Token Budget**: 100-200 tokens per injection
- **Trigger**: Tool name pattern matching (sdk.*)
- **Injection**: Method-specific docs + common pitfalls
- **Performance Target**: <20ms

#### UserPromptSubmit - Keyword-Triggered Help
- **Purpose**: Respond to implicit documentation requests
- **Token Budget**: 200-400 tokens per injection
- **Trigger**: Keyword/intent detection
- **Injection**: Workflow-specific guides
- **Performance Target**: <30ms

### Secondary Hooks (Future Enhancement)

#### PostToolUse - Error-Driven Documentation
- **Purpose**: Inject troubleshooting docs after failures
- **Token Budget**: 150-300 tokens
- **Trigger**: Tool execution errors
- **Injection**: Error-specific resolution guides

#### SessionEnd - Cleanup & Best Practices
- **Purpose**: Reinforce patterns before session closes
- **Token Budget**: 100-150 tokens
- **Trigger**: Session termination
- **Injection**: Session summary tips

---

## 2. Injection Rules with Token Budgets

### SessionStart Injection (150-250 tokens)

**Template:**
```
<htmlgraph_context>
Session {session_id} started.

Quick Reference:
- sdk.features.create("title") - Create feature
- sdk.spikes.create("title") - Create spike
- sdk.analytics.recommend_next_work() - Get recommendations

Active: {X} features, {Y} spikes
Full docs: /htmlgraph-docs skill
</htmlgraph_context>
```

**Implementation Pattern:**
- Check cache first (key: `session_start_{version}`)
- Query active features/spikes count
- Format minimal context (target: 200 tokens)
- Cache for session duration
- Record budget usage

### PreToolUse Injection (100-200 tokens)

**Pattern Matching Map:**
```
sdk.features.create → Feature creation docs (150 tokens)
sdk.spikes.create → Spike creation docs (150 tokens)
sdk.tracks.create → Track creation docs (150 tokens)
sdk.analytics.* → Analytics method docs (180 tokens)
```

**Template Example (sdk.features.create):**
```
<method_docs>
sdk.features.create(title: str) -> FeatureBuilder
  .set_priority("high"|"medium"|"low")
  .set_status("todo"|"in-progress"|"done")
  .add_steps(["step1", "step2"])
  .save() -> Feature

Example:
  feature = sdk.features.create("Auth system")
    .set_priority("high")
    .add_steps(["Design API", "Implement"])
    .save()
</method_docs>
```

**Implementation Pattern:**
- Extract tool name from context
- Fast exit if not SDK tool
- Check cache (key: `tool_{tool_name}`)
- Pattern match against TOOL_DOC_MAP
- Return docs or None
- Cache for 1 hour

### UserPromptSubmit Injection (200-400 tokens)

**Keyword Detection Patterns:**
```
("create", "feature") → Feature creation workflow (280 tokens)
("mark", "complete", "step") → Step completion workflow (250 tokens)
("track", "planning") → Track planning workflow (320 tokens)
("recommend", "next") → Analytics workflow (200 tokens)
```

**Template Example (feature creation):**
```
<workflow_guide>
# Feature Creation Workflow

1. Create: sdk.features.create("title")
2. Set metadata: .set_priority("high")
3. Add steps: .add_steps(["Step 1", "Step 2"])
4. Save: .save()

Example:
  feature = sdk.features.create("User auth")
    .set_priority("high")
    .add_steps(["OAuth", "JWT", "Tests"])
    .save()
</workflow_guide>
```

**Implementation Pattern:**
- Normalize prompt to lowercase
- Check all keywords present (AND logic)
- Don't cache (too variable)
- Record budget usage immediately

---

## 3. Context Budget Management

### Budget Allocation

**Total Budget:** 15,000 tokens (10% of 150k working context)

**Per-Hook Allocations:**
- `session_start`: 250 tokens (one-time)
- `pre_tool_use`: 200 tokens (per invocation)
- `user_prompt`: 400 tokens (per match)
- `post_tool_error`: 300 tokens (per error)
- `reserved`: 2,000 tokens (safety buffer)

### Priority System (Injection Triage)

**When budget approaches limit:**

**Priority Levels:**
1. **Critical (10)**: SessionStart - Always inject
2. **High (8)**: PreToolUse - Just-in-time help
3. **High (7)**: PostToolError - Error recovery
4. **Medium (6)**: UserPrompt - User requested
5. **Low (3)**: SessionEnd - Nice to have

**Eviction Strategy:**
- If budget exceeded and priority >= 8: Evict lowest priority injections
- If budget exceeded and priority < 8: Skip injection
- Monitor utilization: Alert if >80% of budget used

### Budget Tracking

**ContextBudgetManager Class:**
```python
class ContextBudgetManager:
    TOTAL_BUDGET = 15_000
    ALLOCATIONS = {...}

    def can_inject(self, injection_type: str) -> bool:
        # Check if injection would exceed budget
        pass

    def record_injection(self, injection_type: str, tokens: int):
        # Track usage, timestamp
        pass

    def get_utilization(self) -> float:
        # Return percentage used
        pass
```

---

## 4. Dynamic Injection Mechanism

### Hook Implementation Structure

**File:** `packages/claude-plugin/.claude/hooks/documentation-injection.py`

**Core Components:**
1. **DocumentationInjectionHook** - Main hook class
2. **ContextBudgetManager** - Budget tracking
3. **TTLCache** - Doc caching with expiration
4. **PerformanceMonitor** - Latency tracking
5. **TOOL_DOC_MAP** - Method documentation mappings
6. **KEYWORD_WORKFLOW_MAP** - Workflow guide mappings

**Hook Flow:**
```
1. Hook triggered (SessionStart/PreToolUse/UserPromptSubmit)
2. Check budget availability
3. Check cache for matching docs
4. If cache miss: Generate/load docs
5. Record budget usage
6. Return docs (or None if budget exceeded)
```

### Hook Registration

**File:** `packages/claude-plugin/.claude-plugin/plugin.json`

```json
{
  "hooks": [
    {
      "type": "SessionStart",
      "script": "hooks/documentation-injection.py",
      "method": "on_session_start"
    },
    {
      "type": "PreToolUse",
      "script": "hooks/documentation-injection.py",
      "method": "on_pre_tool_use"
    },
    {
      "type": "UserPromptSubmit",
      "script": "hooks/documentation-injection.py",
      "method": "on_user_prompt_submit"
    }
  ]
}
```

---

## 5. Caching Strategy

### Three-Tier Cache System

**Tier 1: LRU Cache (In-Memory)**
- **Maxsize**: 50 entries
- **Eviction**: Least Recently Used
- **Use case**: Frequently accessed SDK method docs
- **Hit rate target**: >80%

**Tier 2: TTL Cache (Time-Based Expiration)**
- **Maxsize**: 100 entries
- **TTL**: 3600 seconds (1 hour)
- **Use case**: Session-specific context
- **Auto-eviction**: Expire stale entries

**Tier 3: Pre-compiled Workflows (Build-Time)**
- **Storage**: Package distribution (`precompiled/` directory)
- **Format**: Minified markdown (whitespace removed)
- **Workflows**: Feature creation, spike creation, track planning, step completion
- **Load time**: Plugin initialization

### Cache Warming

**On Session Start:**
1. Pre-load top 5 SDK methods:
   - features.create
   - spikes.create
   - tracks.create
   - analytics.recommend_next_work
   - features.update

2. Load pre-compiled workflows:
   - feature_creation.txt
   - spike_creation.txt
   - track_planning.txt
   - step_completion.txt

3. Warm cache in background (<100ms)

---

## 6. User Customization

### Override Mechanism

**User Configuration File:** `~/.htmlgraph/custom_docs.yaml`

```yaml
# User-specific documentation overrides
tool_overrides:
  sdk.features.create: |
    <custom_docs>
    # My Custom Feature Creation Pattern

    I always use this pattern:
      feature = sdk.features.create("title")
        .set_priority("high")
        .add_labels(["project-x"])
        .save()
    </custom_docs>

workflow_overrides:
  feature_creation: |
    <custom_workflow>
    # My Workflow
    1. Create feature with project label
    2. Add to current sprint track
    3. Link to Jira ticket
    </custom_workflow>

injection_preferences:
  session_start: true
  pre_tool_use: true
  user_prompt: false  # Disable keyword-triggered
  max_injections_per_session: 10
```

### Custom Hook Subclass

**File:** `.claude/hooks/custom-docs.py` (user's project)

```python
from htmlgraph.hooks import DocumentationInjectionHook

class CustomDocsHook(DocumentationInjectionHook):
    def __init__(self):
        super().__init__()
        self.load_user_overrides()  # Load from ~/.htmlgraph/

    def on_pre_tool_use(self, context):
        # Check user overrides first
        if tool_name in self.user_docs.get('tool_overrides', {}):
            return self.user_docs['tool_overrides'][tool_name]

        # Fallback to package docs
        return super().on_pre_tool_use(context)
```

### Merge Strategy (Package Updates)

**When package updates:**
1. Load package's default docs
2. Load user's custom_docs.yaml (if exists)
3. Merge with user overrides taking precedence
4. User's workflow_overrides replace package defaults
5. User's injection_preferences override package defaults

**No breaking changes** - User customizations preserved across updates.

---

## 7. Performance Targets

### Latency Budgets (Per Hook Type)

| Hook Type | Budget | Target | Critical? |
|-----------|--------|--------|-----------|
| SessionStart | 50ms | 30ms | Yes |
| PreToolUse | 20ms | 10ms | Yes |
| UserPrompt | 30ms | 20ms | No |
| PostToolError | 40ms | 25ms | No |

**Critical Hooks:** Cannot delay tool execution or session start.

### Optimization Strategies

**1. Lazy Loading**
- Don't load docs until pattern matches
- Fast exit for non-SDK tools
- Only parse YAML if cache miss

**2. Pre-computation (Build Time)**
- Minify docs during package build
- Remove comments, extra whitespace
- Store in optimized format

**3. Async Injection (Future)**
- Non-blocking doc loading
- Available by tool completion time
- Requires async hook support

### Performance Monitoring

**PerformanceMonitor Class:**
- Track every injection duration
- Alert if >budget threshold
- Record timing stats for analytics
- Identify slow patterns

**Metrics Tracked:**
- p50, p90, p99 latencies
- Cache hit rate
- Budget utilization over time
- Most injected docs

---

## 8. Example Scenarios with Token Counts

### Scenario 1: Fresh Session Start

**Actions:**
1. SessionStart hook triggers
2. Query active features/spikes (2 features, 1 spike)
3. Format quick reference context
4. Cache for session

**Injection:**
```
<htmlgraph_context>
Session abc-123 started.

Quick Reference:
- sdk.features.create("title") - Create feature
- sdk.spikes.create("title") - Create spike
- sdk.analytics.recommend_next_work() - Get recommendations

Active: 2 features, 1 spike
Full docs: /htmlgraph-docs skill
</htmlgraph_context>
```

**Token Count:** 187 tokens
**Budget Impact:** 187 / 15,000 = **1.2% utilization**

---

### Scenario 2: Agent Calls sdk.features.create()

**Actions:**
1. PreToolUse hook triggers
2. Pattern matches "sdk.features.create"
3. Check cache (miss)
4. Load method docs from TOOL_DOC_MAP
5. Cache with TTL=1h

**Injection:**
```
<method_docs>
sdk.features.create(title: str) -> FeatureBuilder
  .set_priority("high"|"medium"|"low")
  .add_steps(["step1", "step2"])
  .save() -> Feature

Example:
  feature = sdk.features.create("Auth system")
    .set_priority("high")
    .add_steps(["Design", "Implement"])
    .save()
</method_docs>
```

**Token Count:** 143 tokens
**Cumulative:** 330 tokens
**Budget Impact:** 330 / 15,000 = **2.2% utilization**

---

### Scenario 3: User Asks "How do I create a feature?"

**Actions:**
1. UserPromptSubmit hook triggers
2. Keywords detected: ("create", "feature")
3. Match workflow: feature_creation
4. Load from pre-compiled cache

**Injection:**
```
<workflow_guide>
# Feature Creation Workflow

1. Create: sdk.features.create("title")
2. Set metadata: .set_priority("high")
3. Add steps: .add_steps(["Step 1", "Step 2"])
4. Save: .save()

Example:
  feature = sdk.features.create("User auth")
    .set_priority("high")
    .add_steps(["OAuth", "JWT", "Tests"])
    .save()
</workflow_guide>
```

**Token Count:** 287 tokens
**Cumulative:** 617 tokens
**Budget Impact:** 617 / 15,000 = **4.1% utilization**

---

### Scenario 4: Typical 30-Minute Session

**Actions:**
- 1x SessionStart: 187 tokens
- 8x PreToolUse (various SDK calls): 1,200 tokens
- 3x UserPrompt (user questions): 840 tokens

**Total Usage:** 2,227 tokens
**Budget Impact:** 2,227 / 15,000 = **14.8% utilization**

**Remaining Budget:**
- 85 more PreToolUse injections possible
- 42 more UserPrompt injections possible
- Error recovery docs available if needed

---

## 9. Platform-Specific Considerations

### Claude Code (Full Support)

**Available:**
- SessionStart hook
- PreToolUse hook
- UserPromptSubmit hook
- PostToolUse hook
- Full context injection

**Recommendation:** Full implementation as designed.

### Gemini (Limited Support)

**Constraints:**
- Hooks may not be available
- Limited context injection

**Fallback Strategy:**
1. Skill-based docs (`/htmlgraph-docs` skill)
2. Pre-load essential docs in GEMINI.md
3. Context files at session start

**Recommendation:** Provide fallback documentation in GEMINI.md.

### API (No Hook Support)

**Constraints:**
- No hook infrastructure
- No automatic injection

**Fallback Strategy:**
1. Include docs in error responses
2. Client-side SDK injection
3. Documentation links in responses

**Recommendation:** Return relevant docs with error messages.

---

## 10. Implementation Phases

### Phase 1: Core Hooks (Week 1)
- [ ] Implement SessionStart hook
- [ ] Implement PreToolUse hook
- [ ] Basic LRU cache (no TTL)
- [ ] Performance monitoring
- [ ] Unit tests

### Phase 2: User Prompts (Week 2)
- [ ] Implement UserPromptSubmit hook
- [ ] Keyword detection engine
- [ ] Context budget management
- [ ] Integration tests

### Phase 3: Optimization (Week 3)
- [ ] TTL cache implementation
- [ ] Pre-compilation of workflows
- [ ] Cache warming on session start
- [ ] Performance profiling

### Phase 4: Customization (Week 4)
- [ ] User override mechanism
- [ ] custom_docs.yaml schema
- [ ] Merge strategy for updates
- [ ] User documentation

### Phase 5: Advanced Features (Future)
- [ ] PostToolUse error-driven injection
- [ ] Async injection support
- [ ] Platform-specific adapters
- [ ] Injection effectiveness analytics

---

## 11. Success Metrics

### Performance Metrics

**Targets:**
- SessionStart injection: <50ms (stretch: 30ms)
- PreToolUse injection: <20ms (stretch: 10ms)
- UserPrompt injection: <30ms (stretch: 20ms)
- Cache hit rate: >80%

**Monitoring:**
- p50, p90, p99 latencies
- Cache effectiveness
- Budget utilization trends

### Effectiveness Metrics

**Targets:**
- Context utilization: 5-15% of total budget
- User satisfaction: Fewer manual `/help` commands
- Reduced errors: Fewer tool usage mistakes
- Documentation access: >70% via hooks vs manual

**Monitoring:**
- Injection frequency by type
- Most accessed docs
- User disable rate (<5%)

### Adoption Metrics

**Targets:**
- 90%+ of users keep injection enabled
- <5% disable due to performance
- 20%+ create custom docs
- 80%+ benefit from just-in-time docs

---

## 12. Open Questions & Future Work

### Q1: Should we inject docs for non-SDK tools?
- Example: Git workflow docs before git operations?
- **Decision:** Start with SDK-only, expand based on user feedback
- **Rationale:** Keep scope focused, measure effectiveness first

### Q2: How to handle multi-language support?
- Docs currently English-only
- **Future:** i18n support with language detection
- **Implementation:** Locale-specific doc mappings

### Q3: Should injection be opt-in or opt-out?
- **Recommendation:** Opt-out (enabled by default)
- **Rationale:** Immediate value for new users
- **Configuration:** Allow disable in preferences

### Q4: Integration with external documentation systems?
- Example: Fetch docs from company wiki or Confluence
- **Future:** Plugin API for custom doc sources
- **Use case:** Enterprise users with internal docs

### Q5: Telemetry and analytics?
- Track which docs are most injected/useful
- **Privacy:** Opt-in only, anonymized
- **Value:** Improve doc relevance over time

---

## Summary

This hook integration design provides:

1. **Progressive Disclosure** - Start minimal (200 tokens), inject more as needed
2. **Context Efficiency** - <15% budget utilization typical
3. **Performance** - <50ms latency per injection
4. **Customization** - User overrides without breaking package updates
5. **Platform Compatibility** - Full support on Claude Code, graceful degradation elsewhere
6. **Offline-First** - All docs cached locally
7. **Maintainability** - Centralized source (AGENTS.md) with auto-sync

**The system balances token efficiency with useful context, providing just-in-time documentation without overwhelming the agent's context window.**

**Next Steps:**
1. Implement Phase 1 (Core Hooks)
2. Measure actual token usage in real sessions
3. Iterate based on user feedback
4. Expand to additional hooks based on effectiveness

            </div>
        </section>
    </article>
</body>
</html>
